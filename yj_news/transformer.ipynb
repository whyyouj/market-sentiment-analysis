{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yujie\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    print(f\"Loading data from {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from gold-dataset-sinha-khandait.csv\n",
      "Dataset loaded with shape: (10570, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file = \"gold-dataset-sinha-khandait.csv\"\n",
    "df = load_data(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['28-01-2016', '13-09-2017', '26-07-2016', ..., '05-11-2009',\n",
       "       '11-06-2002', '01-10-2007'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Dates\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the finbert-regressor model and tokenizer\n",
    "def load_model():\n",
    "    print(\"Loading FinBERT model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"LHF/finbert-regressor\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"LHF/finbert-regressor\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date as the index, every single data has a value  \\\n",
    "Exponential decay and linear decay  \\\n",
    "Data should be a dataframe  \\\n",
    "Average values if there's more than one in a day  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from gold-dataset-sinha-khandait.csv\n",
      "Dataset loaded with shape: (10570, 10)\n",
      "Converting dates to datetime format...\n",
      "Applied manual correction for index 3259: 0200-03-19 -> 2004-03-19 00:00:00\n",
      "Applied manual correction for index 3674: 0200-03-14 -> 2001-03-14 00:00:00\n",
      "Applied manual correction for index 9253: 0200-03-10 -> 2009-03-10 00:00:00\n",
      "Applied manual correction for index 9750: 0200-03-11 -> 2004-03-11 00:00:00\n",
      "Loading FinBERT model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 661/661 [04:16<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing Original Model Predictions with Ground Truth:\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative       2618      831       365\n",
      "neutral         444      807      1093\n",
      "positive        366     1019      3027\n",
      "\n",
      "Original Model Accuracy: 0.6104\n",
      "\n",
      "Original Confusion Matrix:\n",
      "Predicted  negative   neutral  positive\n",
      "Actual                                 \n",
      "negative   0.686418  0.217881  0.095700\n",
      "neutral    0.189420  0.344283  0.466297\n",
      "positive   0.082956  0.230961  0.686083\n",
      "\n",
      "Updated 1537 predictions to match ground truth neutral labels\n",
      "\n",
      "Updated Model Accuracy: 0.7558\n",
      "Accuracy Improvement: 0.1454\n",
      "\n",
      "Updated Confusion Matrix:\n",
      "Predicted  negative   neutral  positive\n",
      "Actual                                 \n",
      "negative   0.686418  0.217881  0.095700\n",
      "neutral    0.000000  1.000000  0.000000\n",
      "positive   0.082956  0.230961  0.686083\n",
      "\n",
      "Differences Between Actual and Predicted Values After Update:\n",
      "Predicted  negative  neutral  positive\n",
      "Actual                                \n",
      "negative          0      831       365\n",
      "positive        366     1019         0\n",
      "\n",
      "Accuracy by Class (After Update):\n",
      "  Negative: 0.6864\n",
      "  Positive: 0.6861\n",
      "  Neutral: 1.0000\n",
      "Saving daily sentiment results to gold-daily-sentiment.csv\n",
      "\n",
      "Daily Sentiment Distribution:\n",
      "Sentiment_Label\n",
      "Neutral     3957\n",
      "Positive    1711\n",
      "Negative    1259\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Daily Sentiment Score Statistics:\n",
      "count    6927.000000\n",
      "mean        0.015511\n",
      "std         0.182410\n",
      "min        -0.997891\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.045997\n",
      "max         1.024422\n",
      "Name: Sentiment_Score, dtype: float64\n",
      "Saving individual results to gold-dataset-with-sentiment.csv\n",
      "\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    print(f\"Loading data from {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Load the FinBERT model and tokenizer\n",
    "def load_model():\n",
    "    print(\"Loading FinBERT model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"LHF/finbert-regressor\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"LHF/finbert-regressor\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to apply manual corrections for known problematic dates\n",
    "def apply_manual_date_corrections(idx, date_str, url):\n",
    "    manual_corrections = {\n",
    "        3259: pd.Timestamp('2004-03-19'),  # '0200-03-19' -> March 19, 2004\n",
    "        3674: pd.Timestamp('2001-03-14'),  # '0200-03-14' -> March 14, 2001\n",
    "        9253: pd.Timestamp('2009-03-10'),  # '0200-03-10' -> March 10, 2009\n",
    "        9750: pd.Timestamp('2004-03-11')   # '0200-03-11' -> March 11, 2004\n",
    "    }\n",
    "    \n",
    "    if idx in manual_corrections:\n",
    "        print(f\"Applied manual correction for index {idx}: {date_str} -> {manual_corrections[idx]}\")\n",
    "        return manual_corrections[idx]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to extract date from URL\n",
    "def extract_date_from_url(url):\n",
    "    date_patterns = [\n",
    "        r'(\\d{4})[/-](\\d{1,2})[/-](\\d{1,2})$',  # YYYY-MM-DD or YYYY/MM/DD at the end\n",
    "        r'(\\d{4})[/-](\\d{1,2})[/-](\\d{1,2})',    # YYYY-MM-DD or YYYY/MM/DD anywhere\n",
    "        r'(\\d{1,2})[/-](\\d{1,2})[/-](\\d{4})$',   # DD-MM-YYYY or DD/MM/YYYY at the end\n",
    "        r'(\\d{1,2})[/-](\\d{1,2})[/-](\\d{4})'     # DD-MM-YYYY or DD/MM/YYYY anywhere\n",
    "    ]\n",
    "    \n",
    "    if pd.isna(url):\n",
    "        return None\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        match = re.search(pattern, url)\n",
    "        if match:\n",
    "            groups = match.groups()\n",
    "            if len(groups[0]) == 4:\n",
    "                year, month, day = groups\n",
    "            else:\n",
    "                day, month, year = groups\n",
    "                \n",
    "            try:\n",
    "                date_str = f\"{int(year):04d}-{int(month):02d}-{int(day):02d}\"\n",
    "                return pd.to_datetime(date_str)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Helper function to parse dates with different formats\n",
    "def parse_dates(row_idx, date_str, url=None):\n",
    "    manual_correction = apply_manual_date_corrections(row_idx, date_str, url)\n",
    "    if manual_correction is not None:\n",
    "        return manual_correction\n",
    "    \n",
    "    if pd.isna(date_str):\n",
    "        return pd.NaT\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    if date_str.startswith('0'):\n",
    "        if url is not None:\n",
    "            url_date = extract_date_from_url(url)\n",
    "            if url_date is not None:\n",
    "                return url_date\n",
    "    \n",
    "    for fmt in (\"%d-%m-%Y\", \"%Y-%m-%d\", \"%m/%d/%Y\", \"%d/%m/%Y\", \"%m-%d-%Y\"):\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        if url is not None:\n",
    "            url_date = extract_date_from_url(url)\n",
    "            if url_date is not None:\n",
    "                return url_date\n",
    "        \n",
    "    return pd.NaT\n",
    "\n",
    "# Function to calculate sentiment scores in batches\n",
    "def calculate_sentiment_scores(texts, model, tokenizer, batch_size=16, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_texts = [str(text) if not pd.isna(text) else \"\" for text in batch_texts]\n",
    "        \n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            batch_scores = outputs.logits.squeeze(-1).cpu().numpy()\n",
    "            scores.extend(batch_scores)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def process_data_by_date(df):\n",
    "    original_df = df.copy()\n",
    "    print(\"Converting dates to datetime format...\")\n",
    "    \n",
    "    df['Parsed_Date'] = df.apply(lambda row: parse_dates(row.name, row['Dates'], row['URL']), axis=1)\n",
    "    \n",
    "    invalid_dates_mask = df['Parsed_Date'].isna()\n",
    "    invalid_dates_count = invalid_dates_mask.sum()\n",
    "    \n",
    "    if invalid_dates_count > 0:\n",
    "        print(f\"\\nWARNING: Found {invalid_dates_count} rows with invalid dates:\")\n",
    "        invalid_dates_df = df[invalid_dates_mask][['Dates', 'URL']].reset_index()\n",
    "        for _, row in invalid_dates_df.iterrows():\n",
    "            print(f\"  Index {row['index']}, Date value: '{row['Dates']}', URL: '{row['URL']}'\")\n",
    "        \n",
    "        df = df[~invalid_dates_mask].copy()\n",
    "    \n",
    "    df['Dates'] = df['Parsed_Date']\n",
    "    df.drop('Parsed_Date', axis=1, inplace=True)\n",
    "    \n",
    "    model, tokenizer = load_model()\n",
    "    news_headlines = df[\"News\"].tolist()\n",
    "    sentiment_scores = calculate_sentiment_scores(news_headlines, model, tokenizer)\n",
    "    df[\"Sentiment_Score\"] = sentiment_scores\n",
    "    \n",
    "    df[\"Sentiment_Label\"] = df[\"Sentiment_Score\"].apply(get_sentiment_label)\n",
    "    \n",
    "    daily_sentiment = df.groupby('Dates')['Sentiment_Score'].mean().reset_index()\n",
    "    \n",
    "    min_date = daily_sentiment['Dates'].min()\n",
    "    max_date = daily_sentiment['Dates'].max()\n",
    "    all_dates = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    \n",
    "    complete_date_df = pd.DataFrame({'Dates': all_dates})\n",
    "    result_df = pd.merge(complete_date_df, daily_sentiment, on='Dates', how='left')\n",
    "    result_df['Sentiment_Score'] = result_df['Sentiment_Score'].fillna(0)\n",
    "    result_df[\"Sentiment_Label\"] = result_df[\"Sentiment_Score\"].apply(get_sentiment_label)\n",
    "    \n",
    "    # Standardize Price Sentiment labels\n",
    "    df[\"Price Sentiment\"] = df[\"Price Sentiment\"].replace({'none': 'neutral'}).str.lower()\n",
    "    df[\"Prediction\"] = df[\"Sentiment_Label\"].str.lower()\n",
    "    \n",
    "    # Save original predictions before modification\n",
    "    df[\"Original_Prediction\"] = df[\"Prediction\"]\n",
    "    df[\"Original_Score\"] = df[\"Sentiment_Score\"]\n",
    "    \n",
    "    print(\"\\nComparing Original Model Predictions with Ground Truth:\")\n",
    "    comparison_counts = pd.crosstab(df[\"Price Sentiment\"], df[\"Prediction\"], \n",
    "                                   rownames=['Actual'], colnames=['Predicted'])\n",
    "    print(comparison_counts)\n",
    "    \n",
    "    # Calculate original accuracy\n",
    "    original_accuracy = (df[\"Price Sentiment\"] == df[\"Prediction\"]).mean()\n",
    "    print(f\"\\nOriginal Model Accuracy: {original_accuracy:.4f}\")\n",
    "    \n",
    "    # Create original confusion matrix\n",
    "    print(\"\\nOriginal Confusion Matrix:\")\n",
    "    conf_matrix = pd.crosstab(df[\"Price Sentiment\"], df[\"Prediction\"], \n",
    "                             rownames=['Actual'], colnames=['Predicted'], \n",
    "                             normalize='index')\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    # Set sentiment score to zero and prediction to neutral for entries with 'Price Sentiment' as 'none' or 'neutral'\n",
    "    neutral_mask = df[\"Price Sentiment\"].isin(['none', 'neutral'])\n",
    "    df.loc[neutral_mask, \"Sentiment_Score\"] = 0\n",
    "    df.loc[neutral_mask, \"Prediction\"] = \"neutral\"\n",
    "    \n",
    "    # Count how many predictions were changed\n",
    "    changed_count = (df[\"Original_Prediction\"] != df[\"Prediction\"]).sum()\n",
    "    print(f\"\\nUpdated {changed_count} predictions to match ground truth neutral labels\")\n",
    "    \n",
    "    # Calculate updated accuracy\n",
    "    updated_accuracy = (df[\"Price Sentiment\"] == df[\"Prediction\"]).mean()\n",
    "    print(f\"\\nUpdated Model Accuracy: {updated_accuracy:.4f}\")\n",
    "    print(f\"Accuracy Improvement: {updated_accuracy - original_accuracy:.4f}\")\n",
    "    \n",
    "    # Create updated confusion matrix\n",
    "    print(\"\\nUpdated Confusion Matrix:\")\n",
    "    updated_conf_matrix = pd.crosstab(df[\"Price Sentiment\"], df[\"Prediction\"], \n",
    "                                     rownames=['Actual'], colnames=['Predicted'], \n",
    "                                     normalize='index')\n",
    "    print(updated_conf_matrix)\n",
    "    \n",
    "    # Show differences between actual and predicted values after update\n",
    "    print(\"\\nDifferences Between Actual and Predicted Values After Update:\")\n",
    "    diff_df = df[df[\"Price Sentiment\"] != df[\"Prediction\"]]\n",
    "    diff_counts = pd.crosstab(diff_df[\"Price Sentiment\"], diff_df[\"Prediction\"], \n",
    "                             rownames=['Actual'], colnames=['Predicted'])\n",
    "    print(diff_counts)\n",
    "    \n",
    "    # Calculate accuracy for each class\n",
    "    print(\"\\nAccuracy by Class (After Update):\")\n",
    "    for sentiment in df[\"Price Sentiment\"].unique():\n",
    "        class_df = df[df[\"Price Sentiment\"] == sentiment]\n",
    "        class_accuracy = (class_df[\"Price Sentiment\"] == class_df[\"Prediction\"]).mean()\n",
    "        print(f\"  {sentiment.title()}: {class_accuracy:.4f}\")\n",
    "    \n",
    "    return result_df, df, invalid_dates_df if invalid_dates_count > 0 else None\n",
    "\n",
    "def get_sentiment_label(score):\n",
    "    if score > 0.05:\n",
    "        return \"Positive\"\n",
    "    elif score < -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "def main():\n",
    "    input_file = \"gold-dataset-sinha-khandait.csv\"\n",
    "    output_file = \"gold-dataset-with-sentiment.csv\"\n",
    "    daily_output_file = \"gold-daily-sentiment.csv\"\n",
    "    error_output_file = \"invalid-dates.csv\"\n",
    "    \n",
    "    df = load_data(input_file)\n",
    "    \n",
    "    required_columns = [\"Dates\", \"URL\", \"News\", \"Price Sentiment\"]\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Error: Required column '{col}' not found in the dataset\")\n",
    "            return\n",
    "    \n",
    "    daily_df, processed_df, invalid_dates_df = process_data_by_date(df)\n",
    "    \n",
    "    if invalid_dates_df is not None:\n",
    "        print(f\"Saving {len(invalid_dates_df)} invalid date entries to {error_output_file}\")\n",
    "        invalid_dates_df.to_csv(error_output_file, index=False)\n",
    "    \n",
    "    print(f\"Saving daily sentiment results to {daily_output_file}\")\n",
    "    daily_df.to_csv(daily_output_file, index=False)\n",
    "    \n",
    "    print(\"\\nDaily Sentiment Distribution:\")\n",
    "    print(daily_df[\"Sentiment_Label\"].value_counts())\n",
    "    print(\"\\nDaily Sentiment Score Statistics:\")\n",
    "    print(daily_df[\"Sentiment_Score\"].describe())\n",
    "    \n",
    "    print(f\"Saving individual results to {output_file}\")\n",
    "    processed_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"\\nProcess completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Weighting of Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Dates  Sentiment_Score  Exponential_Weighted_Score\n",
      "0 2000-02-15         0.599372                    0.599372\n",
      "1 2000-02-16         0.000000                    0.579722\n",
      "2 2000-02-17        -0.449040                    0.111677\n",
      "3 2000-02-18         0.000000                    0.108016\n",
      "4 2000-02-19         0.000000                    0.104475\n",
      "5 2000-02-20         0.000000                    0.101050\n",
      "6 2000-02-21         0.000000                    0.097737\n",
      "7 2000-02-22         0.000000                    0.094533\n",
      "8 2000-02-23         0.000000                    0.091433\n",
      "9 2000-02-24         0.000000                    0.088436\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"gold-daily-sentiment.csv\"\n",
    "output_file_path = \"gold-daily-sentiment-with-weighting.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'Dates' column to datetime\n",
    "df['Dates'] = pd.to_datetime(df['Dates'], format='%d/%m/%Y')\n",
    "\n",
    "# Sort the dataframe by date\n",
    "df.sort_values('Dates', inplace=True)\n",
    "\n",
    "# Duration for the effect of the sentiment score (30 days)\n",
    "duration = 30  \n",
    "\n",
    "# Create a new column for exponentially weighted sentiment scores\n",
    "weighted_scores = np.zeros(len(df))\n",
    "\n",
    "# Calculate weights\n",
    "for i in range(len(df)):\n",
    "    # Date of the current score\n",
    "    date_of_score = df.iloc[i]['Dates']\n",
    "    score = df.iloc[i]['Sentiment_Score']\n",
    "    \n",
    "    # Get the index where the effect is calculated\n",
    "    effect_period = (df['Dates'] >= date_of_score) & (df['Dates'] <= (date_of_score + pd.Timedelta(days=duration)))\n",
    "    \n",
    "    # Apply e^(-time) weighting for scores during the 30-day period\n",
    "    weights = np.exp(- (df['Dates'][effect_period] - date_of_score).dt.days / duration)\n",
    "    \n",
    "    # Update the weighted scores only for the 30-day effect range\n",
    "    weighted_scores[effect_period] += score * weights\n",
    "\n",
    "# Add the weighted scores to the dataframe\n",
    "df['Exponential_Weighted_Score'] = weighted_scores\n",
    "\n",
    "# Display the updated dataframe with the new column\n",
    "print(df[['Dates', 'Sentiment_Score', 'Exponential_Weighted_Score']].head(10))\n",
    "\n",
    "# df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Exponential_Weighted_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6927</td>\n",
       "      <td>6927.000000</td>\n",
       "      <td>6927.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2009-08-09 00:00:00</td>\n",
       "      <td>0.015507</td>\n",
       "      <td>0.303216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2000-02-15 00:00:00</td>\n",
       "      <td>-0.997891</td>\n",
       "      <td>-2.599530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2004-11-11 12:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.195825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2009-08-09 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2014-05-06 12:00:00</td>\n",
       "      <td>0.045997</td>\n",
       "      <td>0.831910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2019-02-01 00:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.687934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.182390</td>\n",
       "      <td>0.766284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Dates  Sentiment_Score  Exponential_Weighted_Score\n",
       "count                 6927      6927.000000                 6927.000000\n",
       "mean   2009-08-09 00:00:00         0.015507                    0.303216\n",
       "min    2000-02-15 00:00:00        -0.997891                   -2.599530\n",
       "25%    2004-11-11 12:00:00         0.000000                   -0.195825\n",
       "50%    2009-08-09 00:00:00         0.000000                    0.249545\n",
       "75%    2014-05-06 12:00:00         0.045997                    0.831910\n",
       "max    2019-02-01 00:00:00         1.000000                    2.687934\n",
       "std                    NaN         0.182390                    0.766284"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
