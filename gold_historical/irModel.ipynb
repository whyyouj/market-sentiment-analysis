{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regression to deal with missing interest rate values\n",
    "# impute data into interest rates; interest rates are only from 2000 - 2024, but gprices are from 1979 to 2025\n",
    "\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#interest rates factored in\n",
    "\n",
    "# Load the interest rates data\n",
    "ir = pd.read_csv('./gold_historical/real-long-term-rates-2000-2024.csv')\n",
    "\n",
    "# Load the gold prices data\n",
    "gp = pd.read_csv('./gold_historical/Prices_cleaned.csv')\n",
    "\n",
    "# Convert the 'Date' columns to datetime format\n",
    "ir['Date'] = pd.to_datetime(ir['Date'], errors='coerce')\n",
    "gp['Date'] = pd.to_datetime(gp['Date'], errors='coerce')\n",
    "\n",
    "# Set 'Date' as the index for both dataframes\n",
    "ir.set_index('Date', inplace=True)\n",
    "gp.set_index('Date', inplace=True)\n",
    "\n",
    "gp['IR'] = ir['LT-Real-Average']\n",
    "\n",
    "# Create a new column 'Time' as a numerical representation of the date for regression purposes\n",
    "gp['Time'] = np.arange(len(gp))\n",
    "\n",
    "# Separate the data into training (non-missing) and prediction (missing) sets\n",
    "train_data = gp.dropna(subset=['IR'])\n",
    "predict_data = gp[gp['IR'].isnull()]\n",
    "print(train_data.head())\n",
    "print(predict_data.head())\n",
    "\n",
    "# Train a linear regression model on the non-missing data\n",
    "X_train = train_data[['Time']]\n",
    "y_train = train_data['IR']\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict missing values using the trained model\n",
    "X_predict = predict_data[['Time']]\n",
    "predicted_values = model.predict(X_predict)\n",
    "\n",
    "# Fill in the missing values with the predicted values\n",
    "gp.loc[gp['IR'].isnull(), 'IR'] = predicted_values\n",
    "\n",
    "print(gp.head())\n",
    "print(gp.shape)\n",
    "print(gp.iloc[::-1].head())\n",
    "\n",
    "gp.to_csv('./gold_historical/gInterest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using backfill / forwardfill to deal with missing interest rate values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the interest rates data\n",
    "ir = pd.read_csv('./gold_historical/real-long-term-rates-2000-2024.csv')\n",
    "\n",
    "# Load the gold prices data\n",
    "gp = pd.read_csv('./gold_historical/Prices_cleaned.csv')\n",
    "\n",
    "# Convert the 'Date' columns to datetime format\n",
    "ir['Date'] = pd.to_datetime(ir['Date'], errors='coerce')\n",
    "gp['Date'] = pd.to_datetime(gp['Date'], errors='coerce')\n",
    "\n",
    "# Set 'Date' as the index for both dataframes\n",
    "ir.set_index('Date', inplace=True)\n",
    "gp.set_index('Date', inplace=True)\n",
    "\n",
    "gp['IR'] = ir['LT-Real-Average']\n",
    "\n",
    "gp['IR'] = gp['IR'].fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "print(gp.head())\n",
    "print(gp.iloc[::-1].head())\n",
    "print(gp.shape)\n",
    "\n",
    "gp.to_csv('./gold_historical/gInterestFill.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the relationship between interest rate and prices\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./gold_historical/gInterest.csv', usecols=lambda col: col != 'index')\n",
    "# Parse the date and price columns\n",
    "data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Convert the 'Price' column to numeric\n",
    "data['Price'] = data['Price'].str.replace(',', '').astype(float)\n",
    "data['IR'] = data['IR'].astype(float)\n",
    "\n",
    "correlation = data['Price'].corr(data['IR'])\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual model\n",
    "\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "#predict price then calc return\n",
    "\n",
    "data = pd.read_csv('./gold_historical/gInterest.csv', usecols=lambda col: col != 'index')\n",
    "# Parse the date and price columns\n",
    "data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Convert the 'Price' column to numeric\n",
    "data['Price'] = data['Price'].str.replace(',', '').astype(float)\n",
    "data['IR'] = data['IR'].astype(float)\n",
    "\n",
    "# Calculate log returns\n",
    "data['Return'] = np.log(data['Price'] / data['Price'].shift(1))\n",
    "\n",
    "data.dropna(subset=['Return'], inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.size)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data[['Price', 'IR']])\n",
    "\n",
    "\n",
    "train_size = int(len(scaled_data) * 0.95)\n",
    "train_data = scaled_data[0:int(train_size), :]\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "look_back = 14\n",
    "\n",
    "\n",
    "for i in range(look_back, len(train_data)):\n",
    "    X_train.append(train_data[i - look_back: i])\n",
    "    Y_train.append(train_data[i, 0])\n",
    "\n",
    "X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "#X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "testing = scaled_data[train_size - look_back:, :]\n",
    "X_test = []\n",
    "\n",
    "Y_test = scaled_data[train_size:, 0]\n",
    "for i in range(look_back, len(testing)):\n",
    "    X_test.append(testing[i - look_back: i])\n",
    "\n",
    "X_test, Y_test = np.array(X_test), np.array(Y_test)\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "#implanted code\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, return_sequences=True, input_shape=(look_back, 2)))\n",
    "#model.add(keras.layers.LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dense(128, kernel_regularizer=regularizers.L2(0.002)))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "learning_rate = 0.0008\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Implement early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mae', metrics=[RootMeanSquaredError()])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=40, callbacks=[early_stopping, reduce_lr], validation_split=0.2)\n",
    "\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "train = data[:train_size]\n",
    "test = data[train_size:].copy(deep=True)\n",
    "test['pred'] = scaler.inverse_transform(np.concatenate((pred, np.zeros((pred.shape[0], 1))), axis=1))[:, 0]\n",
    "\n",
    "\n",
    "Y_test = Y_test.reshape(-1)\n",
    "test_loss, test_rmse = model.evaluate(X_test, Y_test)\n",
    "\n",
    "print(f\"Test Loss (MAE): {test_loss}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Ensure Y_test is a 2D array before inverse transforming\n",
    "y_test_inv = scaler.inverse_transform(np.concatenate((Y_test.reshape(-1, 1), np.zeros((Y_test.shape[0], 1))), axis=1))[:, 0]\n",
    "\n",
    "# Calculate mean absolute error\n",
    "#mae = mean_absolute_error(y_test_inv.flatten(), test['pred'])\n",
    "mae = mean_absolute_error(y_test_inv, test['pred'])\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(train[\"Price\"], c=\"b\")\n",
    "plt.plot(test[[\"Price\", \"pred\"]])\n",
    "plt.ylabel(\"price\")\n",
    "plt.legend(['train', 'test', 'predic'])\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# Calculate log returns for the predicted prices\n",
    "test['pred_log_return'] = np.log(test['pred'] / test['pred'].shift(1))\n",
    "\n",
    "# Calculate log returns for the actual prices\n",
    "test['actual_log_return'] = np.log(test['Price'] / test['Price'].shift(1))\n",
    "\n",
    "# Drop the first row with NaN values in log returns\n",
    "test = test.dropna()\n",
    "\n",
    "# Calculate mean absolute error and mean percentage error in predicted returns compared to actual returns\n",
    "mae_log_return = mean_absolute_error(test['actual_log_return'], test['pred_log_return'])\n",
    "mape_log_return = mean_absolute_percentage_error(test['actual_log_return'], test['pred_log_return'])\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE) in log returns: {mae_log_return:.6f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE) in log returns: {mape_log_return:.6f}\")\n",
    "\n",
    "# Plot the actual and predicted log returns\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(test.index, test['actual_log_return'], label='Actual Log Return', color='blue')\n",
    "plt.plot(test.index, test['pred_log_return'], label='Predicted Log Return', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Log Return')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Predict tomorrow's price and return\n",
    "last_sequence = scaled_data[-look_back:]\n",
    "last_sequence = np.reshape(last_sequence, (1, look_back, 2))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(last_sequence.shape)\n",
    "# Predict the next price\n",
    "predicted_price_scaled = model.predict(last_sequence)\n",
    "predicted_price = scaler.inverse_transform(np.concatenate((predicted_price_scaled, np.zeros((predicted_price_scaled.shape[0], 1))), axis=1))[:, 0]\n",
    "\n",
    "# Calculate the predicted return for tomorrow\n",
    "last_price = data['Price'].iloc[-1]\n",
    "#predicted_return = (predicted_price[0][0] - last_price) / last_price\n",
    "predicted_return = np.log(predicted_price[0] / last_price)\n",
    "\n",
    "print(f\"Predicted Price for Tomorrow: {predicted_price[0]:.2f}\")\n",
    "print(f\"Predicted Return for Tomorrow: {predicted_return:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if i use forward fill / backward fill for interest rates"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
